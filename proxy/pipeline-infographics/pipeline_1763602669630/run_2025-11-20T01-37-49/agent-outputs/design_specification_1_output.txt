Now I have all the necessary context. Let me create the comprehensive plan for building the performance_analyzer agent.

---

**UNDERSTANDING:** User wants me to design (NOT implement) a `performance_analyzer` agent based on section 2.3 of MINING_CYCLE_AUTOMATION_PLAN.md. The agent should analyze mining performance metrics including hashrate, share acceptance rate, block discovery time, identify bottlenecks, generate reports, and compare to expected values. The agent needs specific permissions (Read, Write, Edit, Bash, Glob, Grep) and must support 4 decisions (analysis_complete, performance_good, performance_issues, error). The final JSON will be written to `/mnt/c/github/claudeplus/agents/performance_analyzer.json`.

**APPROACH:** I will design a comprehensive agent specification by:
1. Analyzing the plan document section 2.3 (lines 356-376) to extract exact requirements
2. Examining similar existing agents (mining_status_checker, performance_test_engineer, balance_analyzer, system_analyzer) to understand the expected JSON structure and system prompt patterns
3. Defining mining-specific performance metrics and analysis criteria
4. Creating decision tree logic for the 4 required decisions
5. Structuring a detailed system prompt with execution workflow, metrics collection, analysis methodology, and reporting format
6. Defining validation criteria and success metrics specific to mining performance

**STEPS:**
1. **Extract Requirements from Plan Document** - Review section 2.3 (lines 356-376) and identify all capabilities: calculate effective hashrate, measure share acceptance rate, track block discovery time, identify bottlenecks, generate performance reports, compare to expected values
2. **Analyze Reference Agents** - Study mining_status_checker.json structure (comprehensive mining-specific prompts), performance_test_engineer.json (performance analysis patterns), balance_analyzer.json (simple structure), system_analyzer.json (analysis workflow patterns)
3. **Define Performance Metrics** - List specific metrics to collect: hashrate (current vs expected), share acceptance rate (%), block discovery time (actual vs theoretical), pool latency, RPC response time, stratum job delivery time, resource utilization
4. **Design Decision Tree** - Create logic for 4 decisions:
   - `analysis_complete`: All metrics collected and analyzed successfully
   - `performance_good`: Metrics within acceptable ranges (e.g., hashrate ≥90%, acceptance ≥95%, no bottlenecks)
   - `performance_issues`: Metrics degraded but operational (e.g., hashrate 50-90%, acceptance 80-95%, minor bottlenecks)
   - `error`: Cannot collect metrics or critical analysis failure
5. **Structure System Prompt** - Build comprehensive prompt sections:
   - Agent role and responsibilities
   - Performance metrics definitions
   - Data collection methodology using Bash, Grep, Read tools
   - Calculation formulas (effective hashrate, acceptance rate, expected block time)
   - Bottleneck identification techniques (CPU, network, RPC, stratum)
   - Decision tree with specific thresholds
   - Report generation format
   - Execution workflow (collect → analyze → compare → identify issues → decide → report)
6. **Define Validation Criteria** - Specify success criteria: hashrate measured, share stats analyzed, block timing tracked, bottlenecks identified, comparison to expected values, decision made, report generated
7. **Create JSON Structure** - Assemble complete agent JSON with: id, name, role, expertise array, systemPrompt (multi-section detailed prompt), outputFormat (markdown), validationCriteria array, decisions array
8. **Specify Tool Usage Patterns** - Document how to use permitted tools:
   - Bash: Execute mining log analysis, process monitoring, network stats
   - Grep: Search logs for hashrate, share submissions, block discoveries, errors
   - Read: Read configuration files for expected values, mining logs
   - Write: Generate performance reports
   - Edit: Update configuration if needed
   - Glob: Find log files across directories

**TEST_CONSIDERATIONS:**
The agent design will handle these test scenarios:
- **Optimal Performance**: All metrics at/above expected values → decision: `performance_good`
- **Degraded Performance**: Metrics below optimal but still functional → decision: `performance_issues` with specific bottleneck identification
- **Multiple Bottlenecks**: Network + CPU issues → report should identify all bottlenecks with priority ranking
- **Missing Expected Values**: No baseline for comparison → use industry standards or warn in report
- **Incomplete Logs**: Cannot find all metrics → decision: `error` with clear explanation
- **Edge Cases**: Zero hashrate, 100% rejection, very long block times → proper categorization in decision tree
- **Comparison Logic**: Handle percentage calculations, time conversions, statistical variance
- **Report Completeness**: Ensure all sections populated even with partial data

**EVIDENCE:**
The plan will include specifications for:
- Complete JSON agent definition file structure
- Detailed system prompt with 8+ major sections (role, metrics, collection, analysis, decision tree, reporting, workflow, examples)
- Decision tree with specific numeric thresholds (e.g., hashrate ≥90% = good, 50-90% = issues, <50% = critical)
- Metrics collection commands (specific Bash/Grep commands for mining logs)
- Performance report template with sections: summary, metrics table, bottleneck analysis, recommendations, decision rationale
- Tool usage examples for each permitted tool (Bash, Grep, Read, Write, Edit, Glob)
- Validation criteria array with 7+ specific items
- Expertise array with 6+ mining performance domains
- Reference to section 2.3 requirements verification checklist

**CONFIDENCE:** High

**Rationale for High Confidence:**
- Clear requirements in section 2.3 of plan document (lines 356-376)
- Strong reference agents available (especially mining_status_checker with comprehensive mining-specific prompts)
- Well-defined performance metrics that are standard in mining operations
- Straightforward decision tree with quantifiable thresholds
- Task is design/planning (not implementation), which is well-suited for specification
- All required components can be designed based on existing patterns and mining domain knowledge
- The 4 decisions map clearly to analysis outcomes (complete, good, issues, error)

[DECISION: design_complete]